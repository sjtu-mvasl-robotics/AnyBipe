Please carefully analyze the feedbacks and provide a new, improved reward function that can better solve the task. Some helpful tips for analyzing the policy feedback:
    (1) If the task score is always near zero, then you must rewrite the entire reward function
    (2) If the values for a certain reward component are near identical throughout, then this means RL is not able to optimize this component as it is written. You may consider
        (a) Changing its scale or the value of its temperature parameter
        (b) Re-writing the reward component 
        (c) Discarding the reward component
    (3) If some reward components' magnitude is significantly larger, then you must re-scale its value to a proper range
    (4) The terrain_level term provides a critical measurement beyond rewards, if your reward is high but terrain_level maintains low scale (less than 2), then the policy may not handle sim2real task well. You should consider re-designing and balancing your reward terms.
Please analyze each existing reward component in the suggested manner above first, and then write the reward function code. 